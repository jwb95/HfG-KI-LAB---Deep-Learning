{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In Tensorflow ist es üblich von der Funktionalität von Keras Gebrauch zu machen und Models durch eine Sequenz von Layer-Objekten zu definieren\n",
    "und dann ein Model-Objekt anhand des Input- und des Output-Layers zu erstellen.\n",
    "Da wir die Variablen, die auf ein Layer deuten, (z.b die Variable 'out' deutet auf ein Dense-Layer) nach Erstellung des Model-Objekts nicht\n",
    "mehr benötigen macht es Sinn die Erstellung des Models in eine Funktion zu packen. So etwa:\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(...)\n",
    "    x = Dense(...)(inp)\n",
    "    ...\n",
    "    out = Dense(...)(x)\n",
    "    model = Model(inp, x)\n",
    "    return model\n",
    "\n",
    "Die Funktion 'build_model()' gibt uns dann nur das Objekt 'model' zurück und alle auf dem Weg dorthin erstellten Variablen werden nach Ausführen\n",
    "der Funktion gelöscht.\n",
    "\n",
    "In PyTorch gibt es nicht direkt ein solches Model-Objekt, stattdessen erstellt man eine sog. 'Klasse'.\n",
    "Machen wir uns also mit den Grundfunktionen von Klassen vertraut.\n",
    "\n",
    "'def' ('definieren/define') ist ja das Keyword, um eine Funktion zu erstellen. Anschließend können wir die Funktion über ihren Namen benutzen, etwa:\n",
    "\"\"\"\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "print(add(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Das Keyword zur Definition einer Klasse ist 'class'.\n",
    "Eine Klasse ist eine Datenstruktur, dessen Funktionalität wir zunächst definieren. Wir können dann theoretisch unendlich viele Instanzen/Objekte einer Klasse \n",
    "erstellen. Klassen besitzen eigene Funktionen, sog. 'Methoden' und zwar mindestens eine, nämlich den Konstruktor. Der Konstruktor ist die Funktion,\n",
    "die aufgerufen wird, wenn wir eine Instanz einer Klasse erstellen wollen. Der Name dieser Konstruktor-Funktion ist in Python immer '__init__()'.\n",
    "Sie hat zwingend ein Argument 'self' und kann wie auch eine Funktion ansonsten beliebige Variablen haben.\n",
    "\"\"\"\n",
    "\n",
    "class Mensch():\n",
    "    def __init__(self, alter, gender):\n",
    "        self.alter = alter\n",
    "        self.gender = gender\n",
    "\n",
    "\"\"\"\n",
    "Der Konstruktor unserer Klasse Mensch nimmt abgesehen von 'self' 2 Argumente (Alter und Geschlecht). Unser Konstruktor speichert diese dann intern im Objekt.\n",
    "self.Alter und self.Geschlecht sind Variablen, die innerhalb des Objektes existieren. Erstellen wir ein Objekt der Klasse 'Mensch'.\n",
    "\"\"\"\n",
    "\n",
    "middleaged_woman = Mensch(47, 'female')\n",
    "\n",
    "\"\"\"\n",
    "Wir können dann auf interne Variablen im Objekt 'middleaged_woman' folgendermaßen zugreifen:\n",
    "\"\"\"\n",
    "\n",
    "print(middleaged_woman.alter)\n",
    "print(middleaged_woman.gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Desweiteren könnten wir unsere Klasse auch etwas spannender gestalten, indem wir weitere Methoden hinzufügen.\n",
    "\"\"\"\n",
    "class Mensch():\n",
    "    def __init__(self, alter, gender):\n",
    "        self.alter = alter\n",
    "        self.gender = gender\n",
    "    \n",
    "    def altern(self, jahre):\n",
    "        self.alter += jahre\n",
    "\n",
    "\"\"\"\n",
    "Diese können wir dann mit selber Syntax wie die Variablen aufrufen.\n",
    "\"\"\"\n",
    "young_diverse = Mensch(22, 'diverse')\n",
    "print(young_diverse.alter)\n",
    "\n",
    "# Methode Altern benutzen.\n",
    "young_diverse.altern(jahre=3)\n",
    "print(young_diverse.alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Eine der wichtigsten Eigenschaften von Klassen ist, dass man sie hierarchisch strukturieren kann. D.h. wenn wir eine Klasse 'Mensch' haben könnten\n",
    "wir außerdem Klassen wie beispielsweise 'Kind', 'Mann', etc. definieren - also solche Klassen, die Unterformen des Überbegriffs 'Mensch' darstellen.\n",
    "Ein 'Kind' hätte genau wie ein 'Mann' auch ein Alter und ein Gender. Auch die Methode altern(), die wir bereits definierten, ergibt für diese Unterklassen Sinn.\n",
    "Deshalb kann man folgendes tun:\n",
    "\"\"\"\n",
    "\n",
    "class Kind(Mensch):\n",
    "    def __init__(self, alter, gender):\n",
    "        super().__init__(alter, gender)\n",
    "\n",
    "\"\"\"\n",
    "Bemerke zunächst, dass wir der Klasse 'Kind' ein Argument übergeben, und zwar die Klasse 'Mensch'. Damit signalisieren wir,\n",
    "dass 'Kind' eine Unterklasse von 'Mensch' ist.\n",
    "Dann schreiben wir in den Konstruktor noch den Ausdruck 'super().__init__(alter, gender)'. Was nun passiert ist, dass 'Kind' den Konstruktor und alle Methoden\n",
    "von Mensch 'erbt'. Wir können also ein 'Kind' erstellen und alle Methoden von Mensch darauf benutzen.\n",
    "\"\"\"\n",
    "\n",
    "female_child = Kind(7, 'female')\n",
    "female_child.altern(jahre=2)\n",
    "print(female_child.alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Kind besucht momentan nicht die Grundschule.\n",
      "Es wird aber so langsam Zeit!\n",
      "Das Kind besucht momentan die Grundschule.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Der Zweck dessen ist, dass die Klasse 'Kind' nun zusätzlich zu den allgemeinen menschlichen Eigenschaften zusätzliche haben könnte,\n",
    "die nicht jedem Menschen zu eigen sind.\n",
    "Gehen wir z.B. davon aus, dass nur Kinder die Grundschule besuchen, wäre so etwas denkbar:\n",
    "\"\"\"\n",
    "\n",
    "class Kind(Mensch):\n",
    "    def __init__(self, alter, gender, besucht_grundschule):\n",
    "        # Durch die folgende Line erbt 'Kind' alle Methoden und den Konstruktor von 'Mensch'.\n",
    "        super().__init__(alter, gender)\n",
    "        # Zusätzlich hat Kind noch das Attribut 'besucht_grundschule'\n",
    "        self.besucht_grundschule = besucht_grundschule\n",
    "    \n",
    "    # Nun können wir noch zusätzliche dem Kind eigene Methoden definieren.\n",
    "    def einschulen(self):\n",
    "        self.besucht_grundschule = True\n",
    "\n",
    "    def ist_grundschuelerin(self):\n",
    "        if self.besucht_grundschule:\n",
    "            print('Das Kind besucht momentan die Grundschule.')\n",
    "        else:\n",
    "            print('Das Kind besucht momentan nicht die Grundschule.')\n",
    "            if self.alter >= 6:\n",
    "                print('Es wird aber so langsam Zeit!')\n",
    "\n",
    "female_child = Kind(6, 'female', False)\n",
    "female_child.ist_grundschuelerin()\n",
    "female_child.einschulen()\n",
    "female_child.ist_grundschuelerin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with PyTorch!\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Warum der Exkurs über Klassen?\n",
    "In PyTorch ist es Konvention Models als Klasse zu definieren, die von der PyTorch-Klasse 'nn.Module' erben.\n",
    "Die Layers eines Models werden durch den Konstruktor im Objekt gespeichert und dann eine Methode\n",
    "'forward()' definiert, die die Layers über einem Input sequentiell ausführt.\n",
    "Definieren wir also eine Encoder-Klasse, die 3 Convolutional Layers mit Strides und ein FC-Layer beinhaltet.\n",
    "\n",
    "Beachte: die Dimensionen in PyTorch folgen einer anderen Konvention als in Tensorflow.\n",
    "In Tensorflow : (batch_dimension, Höhe, Breite, Features)\n",
    "In PyTorch    : (batch_dimension, Features, Höhe, Breite)\n",
    "\"\"\"\n",
    "\n",
    "class Encoder_Model(nn.Module):\n",
    "    # ------------------------------------------------- #\n",
    "    # Konstruktor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Conv (1, 28, 28) -> (16, 14, 14)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, \n",
    "            out_channels=16, \n",
    "            kernel_size=3, \n",
    "            stride=2,\n",
    "            padding=(1,1))\n",
    "\n",
    "        # Conv (16, 14, 14) -> (32, 7, 7)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, \n",
    "            out_channels=32, \n",
    "            kernel_size=3, \n",
    "            stride=2,\n",
    "            padding=(1,1))\n",
    "\n",
    "        # Conv (32, 7, 7) -> (64, 4, 4)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, \n",
    "            out_channels=64, \n",
    "            kernel_size=3, \n",
    "            stride=2,\n",
    "            padding=(1,1))\n",
    "\n",
    "        # FC Layer (576,) -> (9,)\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=4*4*64,\n",
    "            out_features=9)\n",
    "\n",
    "    # ------------------------------------------------- #\n",
    "    # forward-Methode\n",
    "    def forward(self, image_input):\n",
    "        # Conv 1 mit Relu\n",
    "        x = self.conv1(image_input)\n",
    "        x = torch.relu(x)\n",
    "        # Conv 2 mit Relu\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        # Conv 3 mit Relu\n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)\n",
    "        # Flattening der Featuremap, dann Fully-Connected\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        latent_code = self.fc(x)\n",
    "        return latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definieren\n",
    "Encoder = Encoder_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "# Kurzer Test, ob alles wie gewünscht funktioniert. \n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "z = Encoder.forward(x)\n",
    "print(z.shape)\n",
    "\n",
    "# Seems good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bauen wir nun den Decoder als symmetrisches Gegenstück zum Encoder.\n",
    "Wir haben zum Upsamplen zwei Möglichkeiten: UpSampling (Gegenstück zum Pooling) oder sog. Transposed Convolutions/Deconvolutions\n",
    "(Gegenstück zur Strided Convolution). In diesem Notebook wählen wir letztere.\n",
    "Dadurch, dass 28 (die Seitenlänge von MNIST) keine Potenz von 2 ist, ist es mit dem padding und den Kernelgrößen ein gewisses Gepfusche.\n",
    "Allerdings gibt es im Grunde keine zwingenden Argumente dafür, dass ein Model, welches mit etwas 'unregelmäßigen' Parametern gebaut wird,\n",
    "schlechter performen sollte als ein Model, dessen Seitenlängen in den Layern bspw. immer Vielfache von 2 sind.\n",
    "Das ist letzten Endes eine ästhetische Entscheidung zugunsten von Klarheit und Übersicht und ist nicht primär durch Performance motiviert.\n",
    "\"\"\"\n",
    "class Decoder_Model(nn.Module):\n",
    "    # ------------------------------------------------- #\n",
    "    # Konstruktor\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # FC Layer\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=9,\n",
    "            out_features=4*4*64)\n",
    "        \n",
    "        # Conv2Dtrans1\n",
    "        self.convtrans1 = nn.ConvTranspose2d(\n",
    "            in_channels=64, \n",
    "            out_channels=32, \n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=(1,1))\n",
    "\n",
    "        # Conv2Dtrans2\n",
    "        self.convtrans2 = nn.ConvTranspose2d(\n",
    "            in_channels=32, \n",
    "            out_channels=16, \n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=(1,1))\n",
    "\n",
    "        # Conv2Dtrans3\n",
    "        self.convtrans3 = nn.ConvTranspose2d(\n",
    "            in_channels=16, \n",
    "            out_channels=1, \n",
    "            kernel_size=4,\n",
    "            stride=2,\n",
    "            padding=(0,0))\n",
    "\n",
    "    # ------------------------------------------------- #\n",
    "    # forward-Methode\n",
    "    def forward(self, latent_code_input):\n",
    "        # FC, anschließend reshapen wir den Vektor zu einer Matrix (64, 4, 4)\n",
    "        x = self.fc(latent_code_input)\n",
    "        x = torch.relu(x)\n",
    "        x = torch.reshape(x, (x.shape[0], 64, 4, 4))\n",
    "        # Deconvolution 1\n",
    "        x = self.convtrans1(x)\n",
    "        x = torch.relu(x)\n",
    "        # Deconvolution 2\n",
    "        x = self.convtrans2(x)\n",
    "        x = torch.relu(x)\n",
    "        # Deconvolution 3\n",
    "        x = self.convtrans3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definieren\n",
    "Decoder = Decoder_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 9)\n",
    "z = Decoder.forward(x)\n",
    "print(z.shape)\n",
    "\n",
    "# Gut, die Models stehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schnappen wir uns unsere GPU bzw. CPU wenn keine GPU vorhanden ist.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Nun kopieren wir unsere Models auf das device mit der Methode .to\n",
    "Encoder = Encoder.to(device)\n",
    "Decoder = Decoder.to(device)\n",
    "\n",
    "# Den Optimizern in PyTorch übergeben wir die Methode 'parameters' des zu optimierenden Models und die learning rate.\n",
    "opt_encoder = torch.optim.Adam(Encoder.parameters(), lr=1e-3)\n",
    "opt_decoder = torch.optim.Adam(Decoder.parameters(), lr=1e-3)\n",
    "\n",
    "'''\n",
    "Loss-Functions in PyTorch funktionieren wie in Tensorflow. Wir brauchen eine Funktion mit zwei Argumenten (pred, label),\n",
    "die den entsprechenden Fehler errechnet und zurückgibt. Wir wählen für den Autoencoder den 'Mean Squared Error', mit dem wir auch\n",
    "unsere allerersten NNs optimiert haben. Hier bedeutet das, dass für jede Pixelposition der quadrierte Unterschied zwischen dem jeweiligen\n",
    "Pixel im Originalbild/Label und der Rekonstruktion durch das Model berechnet wird. Wir erhalten also 28x28 Skalare. Aus diesen wird dann der\n",
    "'mean'/Durchschnitt berechnet, um einen einzigen Skalar zu produzieren.\n",
    "'''\n",
    "def mse_loss(pred, label):\n",
    "    loss = torch.mean((pred - label)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Laden wir nun MNIST. Die Methode torchvision.datasets.MNIST() gibt uns ein Torch-Dataset-Objekt, welches iterierbar ist und Pillow-Images beinhaltet.\n",
    "(Pillow ist eine Bildlibrary mit eigener Datenstruktur. Analog für Numpy wäre ein Numpy-Array). Wir müssten die Images selbst zu Pytorch-Tensoren konvertieren,\n",
    "doch glücklicherweise gibt es in PyTorch sog. 'transforms', die an die obige Methode übergeben werden können und die Konvertierung für uns automatisieren. \n",
    "'''\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", transform=transform, train=True, download=True)\n",
    "\n",
    "'''\n",
    "Das DataLoader-Objekt in PyTorch ist quasi analog zum Tensorflow-Dataset. Mit num_workers können wir die Anzahl der CPU-Kerne einstellen, die Daten\n",
    "abgreifen und an unser Device weiterleiten.\n",
    "'''\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:32<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/100, loss = 0.019202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:31<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 2/100, loss = 0.019641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:32<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3/100, loss = 0.018457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 151/469 [00:11<00:20, 15.18it/s]"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Lets be cute. Wir machen unsere eigene Progress-Bar mit tqdm.\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    loss_epoch = 0\n",
    "    for batch_images, _ in tqdm(train_loader):\n",
    "        # Batch auf das device laden\n",
    "        batch_images = batch_images.to(device)\n",
    "        # Gradienten resetten.\n",
    "        opt_encoder.zero_grad()\n",
    "        opt_decoder.zero_grad()\n",
    "        # encodieren\n",
    "        batch_latent_codes = Encoder(batch_images)\n",
    "        # decodieren\n",
    "        batch_reconstructions = Decoder(batch_latent_codes)\n",
    "        # loss berechnen\n",
    "        loss = mse_loss(batch_reconstructions, batch_images)\n",
    "        # Gradienten berechnen\n",
    "        loss.backward()\n",
    "        # Gewichte anhand der Gradienten updaten\n",
    "        opt_encoder.step()\n",
    "        opt_decoder.step()\n",
    "        # Loss zum gesamten Loss der Epoche hinzufügen\n",
    "        loss_epoch += loss.item()\n",
    "    \n",
    "    # Durchschnittlichen Loss der Epoche berechnen\n",
    "    loss_epoch = loss_epoch / len(train_loader)\n",
    "    \n",
    "    # Loss der Epochen anzeigen\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, n_epochs, loss))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c72c33be0962ac487280757bb3a776dab5448b4c203a17148c0aa550a18915f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
